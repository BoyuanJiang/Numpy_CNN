{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "from glob import glob\n",
    "from layer.conv import Conv2D\n",
    "from layer.relu import ReLU\n",
    "from layer.linear import Linear\n",
    "from layer.max_pool import MaxPool\n",
    "from layer.softmax_loss import Softmax_and_Loss\n",
    "from layer.bn import BatchNorm2D, BatchNorm1D\n",
    "from layer.dropout import Dropout\n",
    "from utils.average import AverageMeter\n",
    "from random import shuffle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    images_path = glob('./%s/%s*3-ubyte' % (path, kind))[0]\n",
    "    labels_path = glob('./%s/%s*1-ubyte' % (path, kind))[0]\n",
    "\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII',\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_mnist('./data/mnist')\n",
    "test_images, test_labels = load_mnist('./data/mnist', 't10k')\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "init_lr = 0.1\n",
    "step=0\n",
    "train_loss = AverageMeter()\n",
    "train_acc = AverageMeter()\n",
    "val_loss = AverageMeter()\n",
    "val_acc = AverageMeter()\n",
    "train_loss_all=[]\n",
    "val_loss_all=[]\n",
    "train_acc_all=[]\n",
    "val_acc_all=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "conv1 = Conv2D(shape=[batch_size, 1, 32, 32], output_channels=6, ksize=5, stride=1, padding=0)    #28*28\n",
    "bn1 = BatchNorm2D(6)\n",
    "relu1 = ReLU()\n",
    "pool1 = MaxPool(2,2,2)  #14*14\n",
    "conv2 = Conv2D(shape=[batch_size, 6, 14, 14], output_channels=16, ksize=5, stride=1, padding=0)    #10*10\n",
    "bn2 = BatchNorm2D(16)\n",
    "relu2 = ReLU()\n",
    "pool2 = MaxPool(2,2,2)  # 5*5\n",
    "conv3 = Conv2D(shape=[batch_size, 16, 5, 5], output_channels=120, ksize=5, stride=1, padding=0)    #1*1\n",
    "bn3 = BatchNorm2D(120)\n",
    "relu3 = ReLU()\n",
    "fc1 = Linear(1*1*120, 84)\n",
    "relu4 = ReLU()\n",
    "dp = Dropout(0.9)\n",
    "fc2 = Linear(84, 10)\n",
    "sf = Softmax_and_Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.1 2.52645081199735 12.5\n",
      "val loss: 2.200469948664174 val acc: 19.110576923076923\n",
      "0 1 0.1 2.202504631225846 20.3125\n",
      "0 2 0.1 2.0671934659366444 26.5625\n",
      "0 3 0.1 1.8858395453735748 36.197916666666664\n",
      "0 4 0.1 1.7233083283141282 44.3359375\n",
      "0 5 0.1 1.5824771597656961 49.53125\n",
      "0 6 0.1 1.4749512630684725 53.125\n",
      "0 7 0.1 1.3835657530636225 56.138392857142854\n",
      "0 8 0.1 1.29838568566398 59.1796875\n",
      "0 9 0.1 1.216273663931536 62.06597222222222\n",
      "0 10 0.1 1.156227533175099 63.75\n",
      "val loss: 1.8716592991359828 val acc: 65.64503205128206\n",
      "0 11 0.1 0.3579445179334239 89.0625\n",
      "0 12 0.1 0.43947262398912246 87.109375\n",
      "0 13 0.1 0.4632236889357073 86.19791666666667\n",
      "0 14 0.1 0.457557833735648 86.328125\n",
      "0 15 0.1 0.42918186762330424 87.1875\n",
      "0 16 0.1 0.427185731081116 87.36979166666667\n",
      "0 17 0.1 0.40642938059205486 87.83482142857143\n",
      "0 18 0.1 0.4231913469398237 87.3046875\n",
      "0 19 0.1 0.40311505912898504 87.67361111111111\n",
      "0 20 0.1 0.40233763903262976 87.65625\n",
      "val loss: 0.5793050683719027 val acc: 85.09615384615384\n",
      "0 21 0.1 0.5158520014664761 85.9375\n",
      "0 22 0.1 0.3581165168097096 90.625\n",
      "0 23 0.1 0.3464327291861679 90.88541666666667\n",
      "0 24 0.1 0.37939525394791335 89.0625\n",
      "0 25 0.1 0.3592986880883419 89.53125\n",
      "0 26 0.1 0.3509315673176548 89.58333333333333\n",
      "0 27 0.1 0.3458742495996213 89.62053571428571\n",
      "0 28 0.1 0.3312465948598702 89.6484375\n",
      "0 29 0.09000000000000001 0.3199189078906348 90.19097222222223\n",
      "0 30 0.09000000000000001 0.30314455481273905 90.78125\n",
      "val loss: 0.5606160745529354 val acc: 84.52524038461539\n",
      "0 31 0.09000000000000001 0.4694824870697393 87.5\n",
      "0 32 0.09000000000000001 0.37553682002932887 89.84375\n",
      "0 33 0.09000000000000001 0.35148539208603974 90.36458333333333\n",
      "0 34 0.09000000000000001 0.3301203484467975 90.234375\n",
      "0 35 0.09000000000000001 0.32019986250813137 90.0\n",
      "0 36 0.09000000000000001 0.3139610011069935 90.49479166666667\n",
      "0 37 0.09000000000000001 0.3030599719412376 90.84821428571429\n",
      "0 38 0.09000000000000001 0.2925541818055694 90.625\n",
      "0 39 0.09000000000000001 0.2828949942571668 90.97222222222223\n",
      "0 40 0.09000000000000001 0.2801820908441671 91.015625\n",
      "val loss: 0.2266150132595675 val acc: 93.34935897435898\n",
      "0 41 0.09000000000000001 0.19982072841816134 96.09375\n",
      "0 42 0.09000000000000001 0.21823628526419464 94.140625\n",
      "0 43 0.09000000000000001 0.28768095890811535 92.1875\n",
      "0 44 0.09000000000000001 0.2601944238891517 92.3828125\n",
      "0 45 0.09000000000000001 0.22649216896656582 93.28125\n",
      "0 46 0.09000000000000001 0.2426717068973471 92.96875\n",
      "0 47 0.09000000000000001 0.22927587002807281 93.41517857142857\n",
      "0 48 0.09000000000000001 0.2214198589933661 93.65234375\n",
      "0 49 0.09000000000000001 0.21531578445675692 93.83680555555556\n",
      "0 50 0.09000000000000001 0.2075901285233322 93.90625\n",
      "val loss: 0.23287503066436607 val acc: 92.82852564102564\n",
      "0 51 0.09000000000000001 0.18914187426112702 94.53125\n",
      "0 52 0.09000000000000001 0.18662575812209203 93.359375\n",
      "0 53 0.09000000000000001 0.1793524300520881 93.48958333333333\n",
      "0 54 0.09000000000000001 0.18580735644140456 93.5546875\n",
      "0 55 0.09000000000000001 0.19641018016243877 93.59375\n",
      "0 56 0.09000000000000001 0.20686047553620035 93.48958333333333\n",
      "0 57 0.09000000000000001 0.21023208598210788 93.19196428571429\n",
      "0 58 0.09000000000000001 0.2002435837069494 93.5546875\n",
      "0 59 0.08100000000000002 0.20556521014288254 93.57638888888889\n",
      "0 60 0.08100000000000002 0.20761359180621097 93.671875\n",
      "val loss: 0.13773203667881073 val acc: 95.703125\n",
      "0 61 0.08100000000000002 0.17425726812478154 95.3125\n",
      "0 62 0.08100000000000002 0.1366142154810691 96.875\n",
      "0 63 0.08100000000000002 0.15475804061669365 96.61458333333333\n",
      "0 64 0.08100000000000002 0.16409943436667349 95.8984375\n",
      "0 65 0.08100000000000002 0.1604529327250257 96.09375\n",
      "0 66 0.08100000000000002 0.16882647753825195 95.57291666666667\n",
      "0 67 0.08100000000000002 0.16775275942280796 95.3125\n",
      "0 68 0.08100000000000002 0.1708093763017828 95.1171875\n",
      "0 69 0.08100000000000002 0.16971784693576808 95.05208333333333\n",
      "0 70 0.08100000000000002 0.16592996013942501 95.15625\n",
      "val loss: 0.12369639682781688 val acc: 96.24399038461539\n",
      "0 71 0.08100000000000002 0.0937608072580092 96.875\n",
      "0 72 0.08100000000000002 0.0717396274825155 98.046875\n",
      "0 73 0.08100000000000002 0.11328567073617019 97.13541666666667\n",
      "0 74 0.08100000000000002 0.14323723944840316 96.484375\n",
      "0 75 0.08100000000000002 0.15150572947871208 95.78125\n",
      "0 76 0.08100000000000002 0.14818001311271242 95.703125\n",
      "0 77 0.08100000000000002 0.14741022907280413 95.53571428571429\n",
      "0 78 0.08100000000000002 0.1487132583598139 95.5078125\n",
      "0 79 0.08100000000000002 0.15293256341534456 95.39930555555556\n",
      "0 80 0.08100000000000002 0.16506536745043904 95.234375\n",
      "val loss: 0.1736641336607126 val acc: 94.53125\n",
      "0 81 0.08100000000000002 0.10515228091732746 97.65625\n",
      "0 82 0.08100000000000002 0.11113484488444406 97.265625\n",
      "0 83 0.08100000000000002 0.11708308090092544 97.13541666666667\n",
      "0 84 0.08100000000000002 0.11710343716930206 96.484375\n",
      "0 85 0.08100000000000002 0.13524102906448107 95.9375\n",
      "0 86 0.08100000000000002 0.1284257539006298 95.96354166666667\n",
      "0 87 0.08100000000000002 0.1418642827498258 95.08928571428571\n",
      "0 88 0.08100000000000002 0.15780067021658456 95.1171875\n",
      "0 89 0.0729 0.15714627247546809 94.96527777777777\n",
      "0 90 0.0729 0.15313178525394172 95.078125\n",
      "val loss: 0.1278273125714494 val acc: 95.92347756410257\n",
      "0 91 0.0729 0.15198807454717536 96.09375\n",
      "0 92 0.0729 0.16067020421758588 95.703125\n",
      "0 93 0.0729 0.17049413538332522 95.57291666666667\n",
      "0 94 0.0729 0.1908720281233155 95.1171875\n",
      "0 95 0.0729 0.18617772021063544 95.15625\n",
      "0 96 0.0729 0.16615917998028143 95.703125\n",
      "0 97 0.0729 0.16067397115020485 95.75892857142857\n",
      "0 98 0.0729 0.14875008031779427 96.09375\n",
      "0 99 0.0729 0.15528969171996457 95.92013888888889\n",
      "0 100 0.0729 0.1524622189761034 96.09375\n",
      "val loss: 0.11339939588122071 val acc: 96.4443108974359\n",
      "0 101 0.0729 0.09228126414796153 96.875\n",
      "0 102 0.0729 0.1225433889493019 96.09375\n",
      "0 103 0.0729 0.12032088166155963 96.09375\n",
      "0 104 0.0729 0.12261061494418007 95.8984375\n",
      "0 105 0.0729 0.1146474145615047 96.40625\n",
      "0 106 0.0729 0.11956469679517584 96.22395833333333\n",
      "0 107 0.0729 0.11618980284995203 96.20535714285714\n",
      "0 108 0.0729 0.12747429172899805 96.09375\n",
      "0 109 0.0729 0.12632321815857658 96.18055555555556\n",
      "0 110 0.0729 0.12379592227701741 96.25\n",
      "val loss: 0.12440364364037147 val acc: 96.11378205128206\n",
      "0 111 0.0729 0.11703262357369615 95.3125\n",
      "0 112 0.0729 0.10416714569841636 96.09375\n",
      "0 113 0.0729 0.10999116270323867 96.09375\n",
      "0 114 0.0729 0.12161511477211004 95.8984375\n",
      "0 115 0.0729 0.128752667782723 95.9375\n",
      "0 116 0.0729 0.12059320664473701 96.35416666666667\n",
      "0 117 0.0729 0.12547331128048902 96.09375\n",
      "0 118 0.0729 0.13300323759502672 96.09375\n",
      "0 119 0.06561 0.12633863585474855 96.26736111111111\n",
      "0 120 0.06561 0.1276903293802288 96.171875\n",
      "val loss: 0.1163062373026699 val acc: 96.4042467948718\n",
      "0 121 0.06561 0.12244542182963636 96.09375\n",
      "0 122 0.06561 0.1203708277849024 96.484375\n",
      "0 123 0.06561 0.11359554204730943 96.61458333333333\n",
      "0 124 0.06561 0.13535881844825076 95.1171875\n",
      "0 125 0.06561 0.12364008622296718 95.46875\n",
      "0 126 0.06561 0.12476812531825014 95.3125\n",
      "0 127 0.06561 0.12340801981376563 95.53571428571429\n",
      "0 128 0.06561 0.12156881418168125 95.60546875\n",
      "0 129 0.06561 0.12358245701048862 95.57291666666667\n",
      "0 130 0.06561 0.121922848772164 95.703125\n",
      "val loss: 0.09259948599624068 val acc: 97.11538461538461\n",
      "0 131 0.06561 0.12153086602133778 96.875\n",
      "0 132 0.06561 0.09485862048965091 96.875\n",
      "0 133 0.06561 0.11938768651580552 96.09375\n",
      "0 134 0.06561 0.12055505957445106 95.703125\n",
      "0 135 0.06561 0.12393248718362034 95.78125\n",
      "0 136 0.06561 0.11905556987773584 96.09375\n",
      "0 137 0.06561 0.11291746615071603 96.42857142857143\n",
      "0 138 0.06561 0.10732156076131594 96.6796875\n",
      "0 139 0.06561 0.10589858665711187 96.70138888888889\n",
      "0 140 0.06561 0.10051600743894995 96.953125\n",
      "val loss: 0.08712628657225452 val acc: 97.19551282051282\n",
      "0 141 0.06561 0.06503172689595027 97.65625\n",
      "0 142 0.06561 0.13057891635227073 95.703125\n",
      "0 143 0.06561 0.14907892679785556 95.05208333333333\n",
      "0 144 0.06561 0.13763167728600015 95.5078125\n",
      "0 145 0.06561 0.1394283921719866 95.625\n",
      "0 146 0.06561 0.12043108607105524 96.35416666666667\n",
      "0 147 0.06561 0.12920330883392145 96.09375\n",
      "0 148 0.06561 0.11864447600804896 96.2890625\n",
      "0 149 0.05904900000000001 0.11554821506911085 96.44097222222223\n",
      "0 150 0.05904900000000001 0.12757566533016482 96.015625\n",
      "val loss: 0.09476469775293625 val acc: 96.84495192307692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 151 0.05904900000000001 0.1287567815718265 96.09375\n",
      "0 152 0.05904900000000001 0.1349149465774392 96.484375\n",
      "0 153 0.05904900000000001 0.13019128286595802 96.35416666666667\n",
      "0 154 0.05904900000000001 0.1293979053871196 96.2890625\n",
      "0 155 0.05904900000000001 0.13832189646212764 95.9375\n",
      "0 156 0.05904900000000001 0.12610229492589295 96.35416666666667\n",
      "0 157 0.05904900000000001 0.12601915029789976 96.20535714285714\n",
      "0 158 0.05904900000000001 0.12506246979630334 96.19140625\n",
      "0 159 0.05904900000000001 0.11897942515569654 96.35416666666667\n",
      "0 160 0.05904900000000001 0.11073198433489177 96.640625\n",
      "val loss: 0.10172272924199591 val acc: 96.65464743589743\n",
      "0 161 0.05904900000000001 0.1856195828435458 95.3125\n",
      "0 162 0.05904900000000001 0.19137493202075637 94.921875\n",
      "0 163 0.05904900000000001 0.17681191799951712 94.79166666666667\n",
      "0 164 0.05904900000000001 0.16186446864430104 95.3125\n",
      "0 165 0.05904900000000001 0.1491529187500094 95.78125\n",
      "0 166 0.05904900000000001 0.13783950244955404 96.09375\n",
      "0 167 0.05904900000000001 0.12365603816940353 96.42857142857143\n",
      "0 168 0.05904900000000001 0.12031903811258872 96.77734375\n",
      "0 169 0.05904900000000001 0.1240487085498416 96.44097222222223\n",
      "0 170 0.05904900000000001 0.12232960422365076 96.5625\n",
      "val loss: 0.07492671879517451 val acc: 97.54607371794872\n",
      "0 171 0.05904900000000001 0.0789514155305498 96.875\n",
      "0 172 0.05904900000000001 0.09180770602827672 96.875\n",
      "0 173 0.05904900000000001 0.09658739842219964 97.13541666666667\n",
      "0 174 0.05904900000000001 0.0856427960831924 97.4609375\n",
      "0 175 0.05904900000000001 0.09480594708076043 97.03125\n",
      "0 176 0.05904900000000001 0.08587923109124534 97.39583333333333\n",
      "0 177 0.05904900000000001 0.0798437150820325 97.65625\n",
      "0 178 0.05904900000000001 0.09002973238269696 97.36328125\n",
      "0 179 0.05314410000000001 0.08374897785918571 97.56944444444444\n",
      "0 180 0.05314410000000001 0.08423031753013743 97.5\n",
      "val loss: 0.09904589735859516 val acc: 96.85496794871794\n",
      "0 181 0.05314410000000001 0.09602463001906987 97.65625\n",
      "0 182 0.05314410000000001 0.13463328577971664 96.484375\n",
      "0 183 0.05314410000000001 0.15017286730356386 96.61458333333333\n",
      "0 184 0.05314410000000001 0.13366869362490666 97.0703125\n",
      "0 185 0.05314410000000001 0.12458976858024362 97.03125\n",
      "0 186 0.05314410000000001 0.1193295891710963 97.265625\n",
      "0 187 0.05314410000000001 0.12338534016695703 96.875\n",
      "0 188 0.05314410000000001 0.12080499982974105 96.97265625\n",
      "0 189 0.05314410000000001 0.11765960488730455 97.13541666666667\n",
      "0 190 0.05314410000000001 0.11003851238289519 97.265625\n",
      "val loss: 0.08930199126661277 val acc: 97.1854967948718\n",
      "0 191 0.05314410000000001 0.08339367369018265 96.875\n",
      "0 192 0.05314410000000001 0.09831445602313493 97.265625\n",
      "0 193 0.05314410000000001 0.08367131689705033 97.91666666666667\n",
      "0 194 0.05314410000000001 0.09077077367074932 97.4609375\n",
      "0 195 0.05314410000000001 0.08447381373816351 97.65625\n",
      "0 196 0.05314410000000001 0.07525199354547749 97.91666666666667\n",
      "0 197 0.05314410000000001 0.07169025683733429 97.99107142857143\n",
      "0 198 0.05314410000000001 0.08832311597108311 97.16796875\n",
      "0 199 0.05314410000000001 0.08404928570896315 97.22222222222223\n",
      "0 200 0.05314410000000001 0.0839070945180421 97.265625\n",
      "val loss: 0.0839261922581937 val acc: 97.28565705128206\n",
      "0 201 0.05314410000000001 0.06683729903943553 98.4375\n",
      "0 202 0.05314410000000001 0.11417218200126036 96.875\n",
      "0 203 0.05314410000000001 0.11434730132323077 96.61458333333333\n",
      "0 204 0.05314410000000001 0.12151130380609981 96.09375\n",
      "0 205 0.05314410000000001 0.12344796197173305 96.09375\n",
      "0 206 0.05314410000000001 0.11027972968303469 96.61458333333333\n",
      "0 207 0.05314410000000001 0.09917453734869923 96.98660714285714\n",
      "0 208 0.05314410000000001 0.09322920023419304 97.265625\n",
      "0 209 0.04782969000000001 0.09837290652780534 97.13541666666667\n",
      "0 210 0.04782969000000001 0.09663407033566293 97.1875\n",
      "val loss: 0.08180728587564151 val acc: 97.29567307692308\n",
      "0 211 0.04782969000000001 0.1575992844213091 95.3125\n",
      "0 212 0.04782969000000001 0.1635584369359956 96.09375\n",
      "0 213 0.04782969000000001 0.14267526620148716 96.35416666666667\n",
      "0 214 0.04782969000000001 0.14040123942935073 96.09375\n",
      "0 215 0.04782969000000001 0.12312465548048221 96.5625\n",
      "0 216 0.04782969000000001 0.1343388241270794 96.61458333333333\n",
      "0 217 0.04782969000000001 0.1244145851894148 96.65178571428571\n",
      "0 218 0.04782969000000001 0.12145258386549879 96.6796875\n",
      "0 219 0.04782969000000001 0.11592985567638205 96.61458333333333\n",
      "0 220 0.04782969000000001 0.11714793266526993 96.484375\n",
      "val loss: 0.08743392888646385 val acc: 97.23557692307692\n",
      "0 221 0.04782969000000001 0.11364572879269796 95.3125\n",
      "0 222 0.04782969000000001 0.10202391623225997 96.484375\n",
      "0 223 0.04782969000000001 0.10134207206176361 96.35416666666667\n",
      "0 224 0.04782969000000001 0.13526200400390198 96.09375\n",
      "0 225 0.04782969000000001 0.11617000541989578 96.40625\n",
      "0 226 0.04782969000000001 0.10707938382718359 96.35416666666667\n",
      "0 227 0.04782969000000001 0.09885759287671429 96.65178571428571\n",
      "0 228 0.04782969000000001 0.10597682020225463 96.38671875\n",
      "0 229 0.04782969000000001 0.10257053866677686 96.35416666666667\n",
      "0 230 0.04782969000000001 0.10564143101727001 96.171875\n",
      "val loss: 0.08070554519981495 val acc: 97.39583333333333\n",
      "0 231 0.04782969000000001 0.07306033795680504 97.65625\n",
      "0 232 0.04782969000000001 0.0791907449507211 96.875\n",
      "0 233 0.04782969000000001 0.07461884806144256 97.13541666666667\n",
      "0 234 0.04782969000000001 0.066524914772997 97.65625\n",
      "0 235 0.04782969000000001 0.06575978518109665 97.5\n",
      "0 236 0.04782969000000001 0.07213782360317272 97.265625\n",
      "0 237 0.04782969000000001 0.07695954848024225 97.09821428571429\n",
      "0 238 0.04782969000000001 0.07914844125442885 96.97265625\n",
      "0 239 0.04304672100000001 0.07278001807636156 97.30902777777777\n",
      "0 240 0.04304672100000001 0.07150905500370006 97.34375\n",
      "val loss: 0.07602815952069399 val acc: 97.6161858974359\n",
      "0 241 0.04304672100000001 0.05043179647730311 98.4375\n",
      "0 242 0.04304672100000001 0.09086298444022052 96.484375\n",
      "0 243 0.04304672100000001 0.08243854022433111 96.61458333333333\n",
      "0 244 0.04304672100000001 0.08470629468958715 96.875\n",
      "0 245 0.04304672100000001 0.0988972193648449 96.5625\n",
      "0 246 0.04304672100000001 0.0949905272777627 96.61458333333333\n",
      "0 247 0.04304672100000001 0.10823288198223269 96.31696428571429\n",
      "0 248 0.04304672100000001 0.10149967334745628 96.6796875\n",
      "0 249 0.04304672100000001 0.10441940782135019 96.70138888888889\n",
      "0 250 0.04304672100000001 0.09736867174717496 97.03125\n",
      "val loss: 0.07107111292683202 val acc: 97.75641025641026\n",
      "0 251 0.04304672100000001 0.05622226388271388 99.21875\n",
      "0 252 0.04304672100000001 0.09586211140586398 97.265625\n",
      "0 253 0.04304672100000001 0.080672178702356 97.39583333333333\n",
      "0 254 0.04304672100000001 0.06802758806165979 97.8515625\n",
      "0 255 0.04304672100000001 0.06508692666487453 98.125\n",
      "0 256 0.04304672100000001 0.08292979706043888 97.52604166666667\n",
      "0 257 0.04304672100000001 0.07976277577849229 97.76785714285714\n",
      "0 258 0.04304672100000001 0.07556766429157512 97.94921875\n",
      "0 259 0.04304672100000001 0.08259687021578568 97.56944444444444\n",
      "0 260 0.04304672100000001 0.08804660555353822 97.5\n",
      "val loss: 0.06371631767680459 val acc: 98.02684294871794\n",
      "0 261 0.04304672100000001 0.08564467688249323 96.875\n",
      "0 262 0.04304672100000001 0.10309527230422372 97.265625\n",
      "0 263 0.04304672100000001 0.09933135252552801 96.875\n",
      "0 264 0.04304672100000001 0.09752415758578012 96.484375\n",
      "0 265 0.04304672100000001 0.10793168386429856 95.9375\n",
      "0 266 0.04304672100000001 0.1058411053664472 96.22395833333333\n",
      "0 267 0.04304672100000001 0.10287304395746179 96.42857142857143\n",
      "0 268 0.04304672100000001 0.0998344396386374 96.58203125\n",
      "0 269 0.03874204890000001 0.0930657570142127 96.78819444444444\n",
      "0 270 0.03874204890000001 0.089281988124366 96.796875\n",
      "val loss: 0.06514382466213503 val acc: 97.99679487179488\n",
      "0 271 0.03874204890000001 0.2707430245444209 93.75\n",
      "0 272 0.03874204890000001 0.17771500609346447 95.703125\n",
      "0 273 0.03874204890000001 0.1773275249516203 95.57291666666667\n",
      "0 274 0.03874204890000001 0.16465990401828867 95.703125\n",
      "0 275 0.03874204890000001 0.1385946161226006 96.40625\n",
      "0 276 0.03874204890000001 0.13034627266540086 96.35416666666667\n",
      "0 277 0.03874204890000001 0.11812533183487064 96.76339285714286\n",
      "0 278 0.03874204890000001 0.11766811948562576 96.6796875\n",
      "0 279 0.03874204890000001 0.11080767305175734 96.875\n",
      "0 280 0.03874204890000001 0.10372005599900207 97.109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.06571109059740275 val acc: 97.79647435897436\n",
      "0 281 0.03874204890000001 0.06802132941200001 97.65625\n",
      "0 282 0.03874204890000001 0.09018440013114146 96.875\n",
      "0 283 0.03874204890000001 0.09885536337452006 97.13541666666667\n",
      "0 284 0.03874204890000001 0.0890672037387505 97.4609375\n",
      "0 285 0.03874204890000001 0.0855767530604371 97.65625\n",
      "0 286 0.03874204890000001 0.08669295367965917 97.78645833333333\n",
      "0 287 0.03874204890000001 0.09388716297177806 97.32142857142857\n",
      "0 288 0.03874204890000001 0.08905625467492961 97.4609375\n",
      "0 289 0.03874204890000001 0.087672302073593 97.48263888888889\n",
      "0 290 0.03874204890000001 0.08505480568195553 97.421875\n",
      "val loss: 0.06525854946709397 val acc: 97.81650641025641\n",
      "0 291 0.03874204890000001 0.048219583445327424 98.4375\n",
      "0 292 0.03874204890000001 0.06577651775395679 98.046875\n",
      "0 293 0.03874204890000001 0.08124652661377142 97.13541666666667\n",
      "0 294 0.03874204890000001 0.09327991465207441 96.6796875\n",
      "0 295 0.03874204890000001 0.10654584650142869 96.5625\n",
      "0 296 0.03874204890000001 0.09829770984331233 96.74479166666667\n",
      "0 297 0.03874204890000001 0.0933984012256034 96.98660714285714\n",
      "0 298 0.03874204890000001 0.08785706634980285 97.16796875\n",
      "0 299 0.03486784401000001 0.08839365817895779 97.22222222222223\n",
      "0 300 0.03486784401000001 0.08567842499888226 97.34375\n",
      "val loss: 0.060771094382975545 val acc: 98.1270032051282\n",
      "0 301 0.03486784401000001 0.10285122195622567 97.65625\n",
      "0 302 0.03486784401000001 0.08372367932981781 97.65625\n",
      "0 303 0.03486784401000001 0.06472007267187836 98.4375\n",
      "0 304 0.03486784401000001 0.05760647919933562 98.4375\n",
      "0 305 0.03486784401000001 0.0519365276913301 98.4375\n",
      "0 306 0.03486784401000001 0.06100978294546188 97.78645833333333\n",
      "0 307 0.03486784401000001 0.06288840104857045 97.54464285714286\n",
      "0 308 0.03486784401000001 0.06304946653824336 97.65625\n",
      "0 309 0.03486784401000001 0.06509804082413911 97.56944444444444\n",
      "0 310 0.03486784401000001 0.06628119824405203 97.578125\n",
      "val loss: 0.059890895167798955 val acc: 98.15705128205128\n",
      "0 311 0.03486784401000001 0.07940581508810018 96.875\n",
      "0 312 0.03486784401000001 0.0808772179961191 96.875\n",
      "0 313 0.03486784401000001 0.10372371338860671 96.61458333333333\n",
      "0 314 0.03486784401000001 0.09833325314728948 96.875\n",
      "0 315 0.03486784401000001 0.09825252164955046 97.03125\n",
      "0 316 0.03486784401000001 0.09882168931244191 97.265625\n",
      "0 317 0.03486784401000001 0.09873519762059597 97.32142857142857\n",
      "0 318 0.03486784401000001 0.10219027853478346 97.265625\n",
      "0 319 0.03486784401000001 0.10735574809549508 97.13541666666667\n",
      "0 320 0.03486784401000001 0.10562536905043587 97.109375\n",
      "val loss: 0.06547863595579728 val acc: 97.9667467948718\n",
      "0 321 0.03486784401000001 0.06824976431400762 97.65625\n",
      "0 322 0.03486784401000001 0.06633213924102267 98.046875\n",
      "0 323 0.03486784401000001 0.06777764712014418 97.91666666666667\n",
      "0 324 0.03486784401000001 0.07302116160885255 97.265625\n",
      "0 325 0.03486784401000001 0.07167948543888562 97.5\n",
      "0 326 0.03486784401000001 0.07477292822684732 97.39583333333333\n",
      "0 327 0.03486784401000001 0.07059550519770623 97.65625\n",
      "0 328 0.03486784401000001 0.06890257408197947 97.75390625\n",
      "0 329 0.031381059609000006 0.06678513587380104 97.74305555555556\n",
      "0 330 0.031381059609000006 0.06677446213136348 97.8125\n",
      "val loss: 0.05605365004144147 val acc: 98.1270032051282\n",
      "0 331 0.031381059609000006 0.04846761343539473 99.21875\n",
      "0 332 0.031381059609000006 0.07299470664915325 98.4375\n",
      "0 333 0.031381059609000006 0.06112673282213956 98.4375\n",
      "0 334 0.031381059609000006 0.08727642061089677 97.4609375\n",
      "0 335 0.031381059609000006 0.08116485216293862 97.65625\n",
      "0 336 0.031381059609000006 0.08495004100886842 97.65625\n",
      "0 337 0.031381059609000006 0.07795128437942969 97.87946428571429\n",
      "0 338 0.031381059609000006 0.0797909490776489 97.8515625\n",
      "0 339 0.031381059609000006 0.08002444710242003 97.82986111111111\n",
      "0 340 0.031381059609000006 0.0740110855396732 98.046875\n",
      "val loss: 0.05409160265208191 val acc: 98.22716346153847\n",
      "0 341 0.031381059609000006 0.049913452172011284 98.4375\n",
      "0 342 0.031381059609000006 0.046204282353586396 98.4375\n",
      "0 343 0.031381059609000006 0.06438253601848345 97.65625\n",
      "0 344 0.031381059609000006 0.08396276142446223 97.265625\n",
      "0 345 0.031381059609000006 0.07887469865790629 97.34375\n",
      "0 346 0.031381059609000006 0.08924042966347982 97.13541666666667\n",
      "0 347 0.031381059609000006 0.09069015050461722 96.98660714285714\n",
      "0 348 0.031381059609000006 0.0938491609404556 96.6796875\n",
      "0 349 0.031381059609000006 0.09869944759948929 96.52777777777777\n",
      "0 350 0.031381059609000006 0.10514182832819514 96.484375\n",
      "val loss: 0.05522933396712933 val acc: 98.26722756410257\n",
      "0 351 0.031381059609000006 0.11376818258511473 95.3125\n",
      "0 352 0.031381059609000006 0.08771421753828017 96.875\n",
      "0 353 0.031381059609000006 0.08775065039939428 96.875\n",
      "0 354 0.031381059609000006 0.09338570618758535 96.484375\n",
      "0 355 0.031381059609000006 0.08965488958865533 96.5625\n",
      "0 356 0.031381059609000006 0.07839069049986246 97.13541666666667\n",
      "0 357 0.031381059609000006 0.06993011113448443 97.54464285714286\n",
      "0 358 0.031381059609000006 0.06290761830325926 97.8515625\n",
      "0 359 0.028242953648100012 0.06068754816209762 98.00347222222223\n",
      "0 360 0.028242953648100012 0.06704149924325811 97.8125\n",
      "val loss: 0.055793005948421896 val acc: 98.20713141025641\n",
      "0 361 0.028242953648100012 0.040846862885499005 99.21875\n",
      "0 362 0.028242953648100012 0.04790676697409896 98.828125\n",
      "0 363 0.028242953648100012 0.05191104021383559 98.69791666666667\n",
      "0 364 0.028242953648100012 0.06096094330878918 98.4375\n",
      "0 365 0.028242953648100012 0.0748692814722058 97.8125\n",
      "0 366 0.028242953648100012 0.0693103700000591 97.91666666666667\n",
      "0 367 0.028242953648100012 0.06595384258153264 97.99107142857143\n",
      "0 368 0.028242953648100012 0.061005639985540615 98.046875\n",
      "0 369 0.028242953648100012 0.06295830601974184 98.17708333333333\n",
      "0 370 0.028242953648100012 0.06678870716006255 98.125\n",
      "val loss: 0.06260631707424723 val acc: 98.0869391025641\n",
      "0 371 0.028242953648100012 0.11174739498743762 96.875\n",
      "0 372 0.028242953648100012 0.12273613906427477 95.703125\n",
      "0 373 0.028242953648100012 0.09499432330916702 96.35416666666667\n",
      "0 374 0.028242953648100012 0.08125577685395391 97.0703125\n",
      "0 375 0.028242953648100012 0.07581448721366679 97.34375\n",
      "0 376 0.028242953648100012 0.07303450709497254 97.39583333333333\n",
      "0 377 0.028242953648100012 0.08717133907555892 96.76339285714286\n",
      "0 378 0.028242953648100012 0.08655185925699417 96.875\n",
      "0 379 0.028242953648100012 0.0934545876386701 96.875\n",
      "0 380 0.028242953648100012 0.09449314577518075 96.953125\n",
      "val loss: 0.05969124833233272 val acc: 98.1270032051282\n",
      "0 381 0.028242953648100012 0.06280359025795376 97.65625\n",
      "0 382 0.028242953648100012 0.06194666564585956 97.265625\n",
      "0 383 0.028242953648100012 0.06586112344838657 97.39583333333333\n",
      "0 384 0.028242953648100012 0.07648838584545875 97.0703125\n",
      "0 385 0.028242953648100012 0.06787918220996061 97.5\n",
      "0 386 0.028242953648100012 0.06933431248728222 97.52604166666667\n",
      "0 387 0.028242953648100012 0.06839738210080772 97.54464285714286\n",
      "0 388 0.028242953648100012 0.0660246733568911 97.75390625\n",
      "0 389 0.02541865828329001 0.06428876250242656 97.91666666666667\n",
      "0 390 0.02541865828329001 0.06412471829609193 97.96875\n",
      "val loss: 0.06056864687206914 val acc: 98.07692307692308\n",
      "0 391 0.02541865828329001 0.12049992548075539 97.65625\n",
      "0 392 0.02541865828329001 0.09869903477901822 97.65625\n",
      "0 393 0.02541865828329001 0.07557252443506021 98.17708333333333\n",
      "0 394 0.02541865828329001 0.06088407742904237 98.6328125\n",
      "0 395 0.02541865828329001 0.0663962551916951 98.4375\n",
      "0 396 0.02541865828329001 0.07448314747870671 98.046875\n",
      "0 397 0.02541865828329001 0.07199690640500166 98.10267857142857\n",
      "0 398 0.02541865828329001 0.07240368834245103 97.94921875\n",
      "0 399 0.02541865828329001 0.06904003499959643 98.09027777777777\n",
      "0 400 0.02541865828329001 0.07197028443084587 97.96875\n",
      "val loss: 0.05772304143956523 val acc: 98.13701923076923\n",
      "0 401 0.02541865828329001 0.0252729126353406 100.0\n",
      "0 402 0.02541865828329001 0.09008135091930669 97.265625\n",
      "0 403 0.02541865828329001 0.07994816718550923 97.39583333333333\n",
      "0 404 0.02541865828329001 0.10203397558781656 96.875\n",
      "0 405 0.02541865828329001 0.1014295286402996 97.03125\n",
      "0 406 0.02541865828329001 0.0906494578575357 97.39583333333333\n",
      "0 407 0.02541865828329001 0.09388730349344969 97.32142857142857\n",
      "0 408 0.02541865828329001 0.08604806622627877 97.4609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 409 0.02541865828329001 0.0793520539234907 97.65625\n",
      "0 410 0.02541865828329001 0.08253053254905698 97.65625\n",
      "val loss: 0.053368007092551815 val acc: 98.3173076923077\n",
      "0 411 0.02541865828329001 0.08002472982485018 97.65625\n",
      "0 412 0.02541865828329001 0.0658838489652323 97.65625\n",
      "0 413 0.02541865828329001 0.06241997901941009 97.65625\n",
      "0 414 0.02541865828329001 0.060698101778480396 97.8515625\n",
      "0 415 0.02541865828329001 0.05953482567130599 97.96875\n",
      "0 416 0.02541865828329001 0.06899008081667309 97.52604166666667\n",
      "0 417 0.02541865828329001 0.07473061146588092 97.43303571428571\n",
      "0 418 0.02541865828329001 0.07830195560197056 97.4609375\n",
      "0 419 0.02287679245496101 0.07624357193910974 97.65625\n",
      "0 420 0.02287679245496101 0.07275338425383891 97.890625\n",
      "val loss: 0.0534350798104912 val acc: 98.3573717948718\n",
      "0 421 0.02287679245496101 0.12846683416889484 96.09375\n",
      "0 422 0.02287679245496101 0.10159769884664085 96.484375\n",
      "0 423 0.02287679245496101 0.0952733368119144 97.13541666666667\n",
      "0 424 0.02287679245496101 0.10289289162400084 96.875\n",
      "0 425 0.02287679245496101 0.08989299706584601 97.34375\n",
      "0 426 0.02287679245496101 0.07934484151230937 97.78645833333333\n",
      "0 427 0.02287679245496101 0.07026493150557837 98.10267857142857\n",
      "0 428 0.02287679245496101 0.07539990236695876 97.94921875\n",
      "0 429 0.02287679245496101 0.07520503949648633 97.82986111111111\n",
      "0 430 0.02287679245496101 0.0801623573916114 97.65625\n",
      "val loss: 0.05479340878698047 val acc: 98.3173076923077\n",
      "0 431 0.02287679245496101 0.11240426779263482 96.09375\n",
      "0 432 0.02287679245496101 0.08119103176618103 97.65625\n",
      "0 433 0.02287679245496101 0.06973293819176893 97.91666666666667\n",
      "0 434 0.02287679245496101 0.06606672571438235 98.046875\n",
      "0 435 0.02287679245496101 0.0755445850748366 97.5\n",
      "0 436 0.02287679245496101 0.08323621428034288 97.65625\n",
      "0 437 0.02287679245496101 0.07585682593714548 97.87946428571429\n",
      "0 438 0.02287679245496101 0.07806756355119034 97.75390625\n",
      "0 439 0.02287679245496101 0.0757920244204885 97.82986111111111\n",
      "0 440 0.02287679245496101 0.07426993594645157 97.734375\n",
      "val loss: 0.051915985534403984 val acc: 98.34735576923077\n",
      "0 441 0.02287679245496101 0.07903722903622548 98.4375\n",
      "0 442 0.02287679245496101 0.06135132119597364 98.828125\n",
      "0 443 0.02287679245496101 0.055246056476070705 98.95833333333333\n",
      "0 444 0.02287679245496101 0.056167599807833125 98.6328125\n",
      "0 445 0.02287679245496101 0.06683530784253669 98.28125\n",
      "0 446 0.02287679245496101 0.0724749771452107 97.91666666666667\n",
      "0 447 0.02287679245496101 0.06847315994906204 97.99107142857143\n",
      "0 448 0.02287679245496101 0.06490153890013899 98.046875\n",
      "0 449 0.02058911320946491 0.06390965072005358 98.09027777777777\n",
      "0 450 0.02058911320946491 0.06322121005482878 98.125\n",
      "val loss: 0.05034177225499397 val acc: 98.44751602564102\n",
      "0 451 0.02058911320946491 0.13282350181985197 96.875\n",
      "0 452 0.02058911320946491 0.11777387619490251 96.875\n",
      "0 453 0.02058911320946491 0.10819109015161843 97.13541666666667\n",
      "0 454 0.02058911320946491 0.09606468990639833 97.4609375\n",
      "0 455 0.02058911320946491 0.08208983826671255 97.8125\n",
      "0 456 0.02058911320946491 0.0770137951538789 97.91666666666667\n",
      "0 457 0.02058911320946491 0.07781707142391188 97.76785714285714\n",
      "0 458 0.02058911320946491 0.07590090783198755 97.8515625\n",
      "0 459 0.02058911320946491 0.07818253625899192 97.82986111111111\n",
      "0 460 0.02058911320946491 0.07676970911841877 97.8125\n",
      "val loss: 0.04985427529829588 val acc: 98.4775641025641\n",
      "0 461 0.02058911320946491 0.024623288569462935 100.0\n",
      "0 462 0.02058911320946491 0.033602561228327124 99.609375\n",
      "0 463 0.02058911320946491 0.03638784466953842 98.95833333333333\n",
      "0 464 0.02058911320946491 0.03808752194930587 98.6328125\n",
      "0 465 0.02058911320946491 0.03690611185949485 98.75\n",
      "0 466 0.02058911320946491 0.04478271090188099 98.69791666666667\n",
      "0 467 0.02058911320946491 0.04684569076262752 98.66071428571429\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    index = [i for i in range(images.shape[0])]\n",
    "    shuffle(index)\n",
    "    images = images[index]\n",
    "    labels = labels[index]\n",
    "    # train\n",
    "    for i in range(images.shape[0] // batch_size):\n",
    "        step+=1\n",
    "        # learning rate decay\n",
    "        lr = init_lr*0.9**(step//30)\n",
    "        # forward\n",
    "        img = images[i * batch_size:(i + 1) * batch_size].reshape([batch_size, 28, 28, 1]).transpose(0,3,1,2)\n",
    "        img = np.pad(img, [[0,0],[0,0],[2,2],[2,2]], mode='constant')/255.0\n",
    "        img = (img-0.1307)/0.3081\n",
    "        label = labels[i * batch_size:(i + 1) * batch_size]\n",
    "        out, conv1_cache = conv1.forward(img)\n",
    "        out, bn1_cache = bn1.forward(out)\n",
    "        out, relu1_cache = relu1.forward(out)\n",
    "        out, pool1_cache = pool1.forward(out)\n",
    "\n",
    "        out, conv2_cache = conv2.forward(out)\n",
    "        out, bn2_cache = bn2.forward(out)\n",
    "        out, relu2_cache = relu2.forward(out)\n",
    "        out, pool2_cache = pool2.forward(out)\n",
    "\n",
    "        out, conv3_cache = conv3.forward(out)\n",
    "        out, bn3_cache = bn3.forward(out)\n",
    "        out, relu3_cache = relu3.forward(out)\n",
    "\n",
    "        conv_out = out\n",
    "\n",
    "        out = conv_out.reshape(batch_size, -1)\n",
    "        out, fc1_cache = fc1.forward(out)\n",
    "        out, relu4_cache = relu4.forward(out)\n",
    "        out, dp_cache = dp.forward(out)\n",
    "\n",
    "        out, fc2_cache = fc2.forward(out)\n",
    "        loss, dx =  sf.forward_and_backward(out, np.array(label))\n",
    "\n",
    "        # calculate gradient\n",
    "        _, _, dx = fc2.gradient(dx, fc2_cache)\n",
    "\n",
    "        dx = dp.gradient(dx, dp_cache)\n",
    "        dx = relu4.gradient(dx, relu4_cache)\n",
    "        _,_,dx = fc1.gradient(dx, fc1_cache)\n",
    "\n",
    "        dx = dx.reshape(conv_out.shape)\n",
    "\n",
    "        dx = relu3.gradient(dx, relu3_cache)\n",
    "        _,_,dx = bn3.gradient(dx, bn3_cache)\n",
    "        _,_,dx = conv3.gradient(dx, conv3_cache)\n",
    "\n",
    "\n",
    "        dx = pool2.gradient(dx, pool2_cache)\n",
    "        dx = relu2.gradient(dx, relu2_cache)\n",
    "        _, _, dx = bn2.gradient(dx, bn2_cache)\n",
    "        _, _ ,dx = conv2.gradient(dx, conv2_cache)\n",
    "\n",
    "        dx = pool1.gradient(dx, pool1_cache)\n",
    "        dx = relu1.gradient(dx, relu1_cache)\n",
    "        _, _, dx = bn1.gradient(dx, bn1_cache)\n",
    "        _,_, dx = conv1.gradient(dx, conv1_cache)\n",
    "\n",
    "        # backward\n",
    "        conv1.backward(lr)\n",
    "        bn1.backward(lr)\n",
    "        conv2.backward(lr)\n",
    "        bn2.backward(lr)\n",
    "        conv3.backward(lr)\n",
    "        bn3.backward(lr)\n",
    "        fc1.backward(lr)\n",
    "        fc2.backward(lr)\n",
    "        \n",
    "        \n",
    "        pred = np.argmax(out, axis=1)\n",
    "        correct = pred.__eq__(label).sum()\n",
    "        train_acc.update(correct/label.size*100)\n",
    "        train_loss.update(loss)\n",
    "\n",
    "\n",
    "        print(epoch,i,lr, train_loss.avg,train_acc.avg)\n",
    "\n",
    "        if i%10==0:\n",
    "            for k in range(test_images.shape[0] // batch_size):\n",
    "                batch_acc = 0\n",
    "                img = test_images[k * batch_size:(k + 1) * batch_size].reshape([batch_size, 28, 28, 1]).transpose(0, 3, 1, 2)\n",
    "                img = np.pad(img, [[0, 0], [0, 0], [2, 2], [2, 2]], mode='constant')/255.0\n",
    "                img = (img-0.1307)/0.3081\n",
    "                label = test_labels[k * batch_size:(k + 1) * batch_size]\n",
    "\n",
    "                out, conv1_cache = conv1.forward(img)\n",
    "                out, bn1_cache = bn1.forward(out, False)\n",
    "                out, relu1_cache = relu1.forward(out)\n",
    "                out, pool1_cache = pool1.forward(out)\n",
    "\n",
    "                out, conv2_cache = conv2.forward(out)\n",
    "                out, bn2_cache = bn2.forward(out, False)\n",
    "                out, relu2_cache = relu2.forward(out)\n",
    "                out, pool2_cache = pool2.forward(out)\n",
    "\n",
    "                out, conv3_cache = conv3.forward(out)\n",
    "                out, bn3_cache = bn3.forward(out, False)\n",
    "                out, relu3_cache = relu3.forward(out)\n",
    "\n",
    "                conv_out = out\n",
    "\n",
    "                out = conv_out.reshape(batch_size, -1)\n",
    "                out, fc1_cache = fc1.forward(out)\n",
    "                out, relu4_cache = relu4.forward(out)\n",
    "                out = dp.forward(out, False)\n",
    "\n",
    "                out, fc2_cache = fc2.forward(out)\n",
    "                loss, dx =  sf.forward_and_backward(out, np.array(label))\n",
    "                \n",
    "                pred = np.argmax(out, axis=1)\n",
    "                correct = pred.__eq__(label).sum()\n",
    "                val_acc.update(correct/label.size*100)\n",
    "                val_loss.update(loss)\n",
    "            print(\"val loss:\", val_loss.avg, \"val acc:\", val_acc.avg)\n",
    "            \n",
    "            train_acc_all.append(train_acc.avg)\n",
    "            train_loss_all.append(train_loss.avg)\n",
    "            val_acc_all.append(val_acc.avg)\n",
    "            val_loss_all.append(val_loss.avg)\n",
    "            \n",
    "            val_loss.reset()\n",
    "            val_acc.reset()\n",
    "            train_acc.reset()\n",
    "            train_loss.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c4170fa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(len(train_acc_all))*10,train_acc_all, label=\"train\")\n",
    "ax.plot(np.arange(len(val_acc_all))*10,val_acc_all, label=\"test\")\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(np.arange(len(train_loss_all))*10,train_loss_all, label=\"train\")\n",
    "ax2.plot(np.arange(len(val_loss_all))*10,val_loss_all, label=\"test\")\n",
    "ax.legend(loc='center right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
